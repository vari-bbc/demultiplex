import pandas as pd
import numpy as np
import io
#from cStringIO import StringIO
import os
import re
from snakemake.utils import validate, min_version
##### set minimum snakemake version #####
min_version("5.28.0")

configfile: "demultiplex/bcl2fastq_snake/config.yaml"

# minimum reads to be considered a low read fastq
min_reads = config['min_reads'] #100

# Reference name for complexity
complexity_ref_name = config['complexity']['ref_name']

# https://stackoverflow.com/questions/10717504/is-it-possible-to-use-read-csv-to-read-only-specific-lines
s = io.StringIO()
read_lens = []
with open('SampleSheet.csv') as f:
    skip_line = True
    reads_section = False
    for line in f:
        # following two if blocks for parsing stuff below [Data]
        if not skip_line:
            s.write(line)
        if line.startswith('[Data]'):
            skip_line = False
        # following code for extracting out the read lengths to determine SE or PE
        if line.startswith('[Reads]'):
            reads_section = True
        elif line.startswith('['):
            reads_section = False
        if reads_section:
            read_len_search = re.search('^(\d+)', line)
            if read_len_search:
                read_lens.append(read_len_search.group(1))
s.seek(0) # "rewind" to the beginning of the StringIO object

# read in the csv with the header lines removed
samples = pd.read_csv(s)


# Next we add a SampleNum column based on order of appearance in the Sample_Name column
sample_names = samples.Sample_Name.values
uniq_samples = pd.unique(sample_names).tolist()

samples['SampleNum'] = [uniq_samples.index(sampleName) for sampleName in sample_names]
samples['SampleNum'] = samples['SampleNum'] + 1

# check that either 1 or 2 read lengths parsed from SampleSheet.csv
assert (0 < len(read_lens) < 3), "Number of read lengths parsed not 1 or 2. It was " + str(len(read_lens))

# whether data is PE (or SE)
paired_end = True if len(read_lens) == 2 else False

if 'Lane' in samples.columns:
    lanes = pd.unique(samples['Lane'])
    sample_fqs = expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{sample.Lane}_R{read}_001.fastq.gz", sample=samples.itertuples(), read=[1,2] if paired_end else [1]),
    undetermined_fqs = expand("Data/Intensities/BaseCalls/Undetermined_S0_L00{lane}_R{read}_001.fastq.gz", read=[1,2] if paired_end else [1], lane=lanes)
else:
    lanes = [1,2,3,4] # if no Lane column in SampleSheet.csv, assume it is NextSeq and that each sample spread over 4 lanes.
    sample_fqs = expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{lane}_R{read}_001.fastq.gz", sample=samples.itertuples(), read=[1,2] if paired_end else [1], lane=lanes),
    undetermined_fqs = expand("Data/Intensities/BaseCalls/Undetermined_S0_L00{lane}_R{read}_001.fastq.gz", read=[1,2] if paired_end else [1], lane=lanes)


missing_fq_yamls = expand("Data/Intensities/BaseCalls/{project}/missing_fastqs_mqc.yaml", project=np.unique(samples['Sample_Project']))
low_counts_fq_yamls = expand("Data/Intensities/BaseCalls/{project}/low_counts_fastqs_mqc.yaml", project=np.unique(samples['Sample_Project']))
#fastqc_and_screen_out = expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/FastQC/{sample.Sample_Name}_L000_R{read}_001_{prog}.html", sample=samples.itertuples(), read=[1,2] if paired_end else [1], prog=['fastqc','screen']) + expand("Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001_{prog}.html", read=[1,2] if paired_end else [1], prog=['fastqc','screen'])

rule all:
    input:
        sample_fqs,
        undetermined_fqs,
        expand("Data/Intensities/BaseCalls/{project}/multiqc_report.html", project=pd.unique(samples['Sample_Project'])), #if "run_qc" in config else [],
        expand("Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001.fastq.gz", read=[1,2]),
        #fastqc_and_screen_out if "run_qc" in config else [],
        #expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/FastQC/{sample.Sample_Name}_L000_R{read}_001_{prog}.html", sample=samples.itertuples(), read=[1,2] if paired_end else [1], prog=['fastqc','screen']),
        #expand("Data/Intensities/BaseCalls/Undetermined_S0_L000_R{read}_001_{prog}.html", read=[1,2] if paired_end else [1], prog=['fastqc','screen']),
        #expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{sample.Lane}_R{read}_001.fastq.gz", sample=samples.itertuples(), read=[1,2] if paired_end else [1])

rule bcl2fastq:
    """
    Run bcl2fastq.
    """
    input:
    output:
        done="bcl2fastq.done",
        sample_fqs=sample_fqs,
        undetermined_fqs=undetermined_fqs,
        missing_fastqs_yaml=missing_fq_yamls,
        missing_fastqs_log="missing_fastqs.log",
        #sample_fqs=expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{sample.Lane}_R{read}_001.fastq.gz", sample=samples.itertuples(), read=[1,2] if paired_end else [1]),
        #undetermined_fqs=expand("Data/Intensities/BaseCalls/Undetermined_S0_L00{lane}_R{read}_001.fastq.gz", read=[1,2] if paired_end else [1], lane=pd.unique(samples['Lane']))
    params:
    log:
        log="bcl2fastq.log",
        stdout="snakemake_job_logs/bcl2fastq/out.o",
        stderr="snakemake_job_logs/bcl2fastq/out.e"
    benchmark:
        "snakemake_job_logs/benchmarks/bcl2fastq/benchmark.txt"
    envmodules:
        "bbc/bcl2fastq2/bcl2fastq2-2.20.0"
    threads: 8
    resources:
        mem_gb = 180
    shell:
        """
        bcl2fastq &> {log.log}
       
        touch {output.missing_fastqs_log}

        # Add header lines for the missing fastqs yaml
        for yaml_file in {output.missing_fastqs_yaml}
        do
            echo "id: 'missing-fqs'\nsection_name: 'Empty Fastq files after dmux'\ndescription: 'This section lists all the fastq files that ended up having 0 reads after bcl2fastq.'\nplot_type: 'html'\ndata: |\n    <ul>" > $yaml_file
        done

        # Add missing files to the missing fastqs yaml files
        for fastq in {output.sample_fqs}
        do
            if [[ ! -f $fastq ]]
            then
            echo "      <li>`basename $fastq`</li>" >> `dirname $fastq`'/missing_fastqs_mqc.yaml'
            echo $fastq >> {output.missing_fastqs_log}
            touch $fastq

            fi
        done
        
        # Add footer line for the missing fastqs yaml
        for yaml_file in {output.missing_fastqs_yaml}
        do
            echo "    </ul>" >> $yaml_file
        done

        touch {output.done}
        """

def get_all_lanes_fqs(wildcards):
    if 'Lane' in samples.columns:
        fqs = expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{sample.Lane}_R{read}_001.fastq.gz", sample=samples[samples['Sample_Name'] == wildcards.sample].itertuples(), read=wildcards.read)
    else:
        fqs = expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/{sample.Sample_Name}_S{sample.SampleNum}_L00{lane}_R{read}_001.fastq.gz", sample=samples[samples['Sample_Name'] == wildcards.sample].itertuples(), read=wildcards.read, lane=lanes)

    return fqs

rule cat_fqs:
    """
    Merge lanes for sample fastq.gz files.
    """
    input:
        get_all_lanes_fqs
        
    output:
        "Data/Intensities/BaseCalls/{project}/{sample}_L000_R{read}_001.fastq.gz"
    params:
    log:
        stdout="snakemake_job_logs/cat_fqs/{project}/{sample}_R{read}.o",
        stderr="snakemake_job_logs/cat_fqs/{project}/{sample}_R{read}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/cat_fqs/{project}/{sample}_R{read}.txt"
    envmodules:
    threads: 1
    resources:
        mem_gb = 22
    run:
        if len(input) > 1:
            shell("cat {input} > {output}")
        else:
            shell("ln -sr {input} {output}")

rule cat_fqs_undetermined:
    """
    Merge lanes for undetermined fastq.gz files.
    """
    input:
        expand("Data/Intensities/BaseCalls/Undetermined_S0_L00{lane}_R{{read}}_001.fastq.gz", lane=lanes)
    output:
        "Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001.fastq.gz"
    params:
    log:
        stdout="snakemake_job_logs/cat_fqs_undetermined/Undetermined_L000_R{read}.o",
        stderr="snakemake_job_logs/cat_fqs_undetermined/Undetermined_L000_R{read}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/cat_fqs_undetermined/Undetermined_L000_R{read}.txt"
    envmodules:
    threads: 1
    resources:
        mem_gb = 22
    run:
        if len(input) > 1:
            shell("cat {input} > {output}")
        else:
            shell("ln -sr {input} {output}")

# rule symlink_undet_to_each_proj:
#     """
#     Symlink Undetermined files to project directories.
#     """
#     input:
#         expand("Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001{suffix}", read=[1,2] if paired_end else [1], suffix=['_fastqc.html','_screen.html','_fastqc.zip','_screen.txt','.fastq.gz']),
#         expand("Data/Intensities/BaseCalls/Undetermined_S0_L00{lane}_R{read}_001.fastq.gz", read=[1,2] if paired_end else [1], lane=lanes),
#     output:
#         expand("Data/Intensities/BaseCalls/{{project}}/Undetermined_L000_R{read}_001{suffix}", read=[1,2] if paired_end else [1], suffix=['_fastqc.html','_screen.html','_fastqc.zip','_screen.txt','.fastq.gz']),
#         expand("Data/Intensities/BaseCalls/{{project}}/Undetermined_S0_L00{lane}_R{read}_001.fastq.gz", read=[1,2] if paired_end else [1], lane=lanes),
#     log:
#         stdout="snakemake_job_logs/symlink_undet_to_each_proj/{project}.o",
#         stderr="snakemake_job_logs/symlink_undet_to_each_proj/{project}.e"
#     benchmark:
#         "snakemake_job_logs/benchmarks/symlink_undet_to_each_proj/{project}.txt"
#     params:
#         project = "{project}"
#     threads: 1
#     resources:
#         mem_gb=32
#     envmodules:
#     shell:
#         """
#         ln -sr Data/Intensities/BaseCalls/Undetermined* Data/Intensities/BaseCalls/{params.project}
#
#         """


def get_post_alignment_metrics(wildcards):
    input_files = []
    if config['complexity']['run']:

        align_dir = "Data/Intensities/BaseCalls/{project}/Align/{aligner}".format(project=wildcards.project, aligner = 'star' if config['complexity']['DNA_or_RNA']=='RNA' else 'bwa')
        sample_names = samples[samples['Sample_Project'] == wildcards.project]['Sample_Name']
        #aligner_suff = '' if config['complexity']['DNA_or_RNA']=="DNA" else '.Aligned.sortedByCoord.out'
        #input_files = expand(align_dir + "/flagstat/{sample}{aligner_suff}.flagstat", sample=sample_names, aligner_suff=aligner_suff)
        #input_files = input_files + expand(align_dir + "/CollectInsertSizeMetrics/{sample}{aligner_suff}.insert_size_metrics.txt", sample=sample_names, aligner_suff=aligner_suff)
        input_files = expand(align_dir + "/CollectAlignmentSummaryMetrics/{sample}.bam.aln_metrics", sample=sample_names) if config['complexity']['DNA_or_RNA']=='DNA' else []

    return input_files


rule multiqc:
    """
    Make multiQC report.
    """
    input:
        lambda wildcards: expand("Data/Intensities/BaseCalls/{{project}}/FastQC/{sample.Sample_Name}_L000_R{read}_001_{prog}.html", sample=samples[samples['Sample_Project'] == wildcards.project].itertuples(), read=[1,2] if paired_end else [1], prog=['fastqc','screen']),
        #expand("Data/Intensities/BaseCalls/{{project}}/Undetermined_L000_R{read}_001_{prog}.html", read=[1,2] if paired_end else [1], prog=['fastqc','screen']),
        lambda wildcards: expand("Data/Intensities/BaseCalls/{{project}}/Align/preseq_complexity/{sample.Sample_Name}.lc_extrap.txt", sample=samples[samples['Sample_Project'] == wildcards.project].itertuples()) if config['complexity']['run'] else [],
        low_counts_fq_yamls,
        get_post_alignment_metrics,
    output:
        "Data/Intensities/BaseCalls/{project}/multiqc_report.html",
        directory("Data/Intensities/BaseCalls/{project}/multiqc_data")
    log:
        stdout="snakemake_job_logs/multiqc/{project}.o",
        stderr="snakemake_job_logs/multiqc/{project}.e",
    benchmark:
        "snakemake_job_logs/benchmarks/multiqc/{project}.txt"
    params:
        project_dir="Data/Intensities/BaseCalls/{project}/",
    threads: 1
    resources:
        mem_gb=32
    envmodules:
        "bbc/multiqc/multiqc-1.9"
    shell:
        """
        multiqc -f \
        -o {params.project_dir} {params.project_dir}

        """


rule fastqc:
    """
    Run fastqc for merged (across lanes) sample files.
    """
    input:
        "Data/Intensities/BaseCalls/{project}/{sample}_L000_R{read}_001.fastq.gz"
    output:
        html="Data/Intensities/BaseCalls/{project}/FastQC/{sample}_L000_R{read}_001_fastqc.html",
        files=directory("Data/Intensities/BaseCalls/{project}/FastQC/{sample}_L000_R{read}_001_fastqc"),
        done=touch("Data/Intensities/BaseCalls/{project}/FastQC/{sample}_L000_R{read}_001_fastqc/done")
    params:
        outdir="Data/Intensities/BaseCalls/{project}/FastQC/"
    log:
        stdout="snakemake_job_logs/fastqc/{project}/{sample}_R{read}.o",
        stderr="snakemake_job_logs/fastqc/{project}/{sample}_R{read}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/fastqc/{project}/{sample}_R{read}.txt"
    envmodules:
        "bbc/fastqc/fastqc-0.11.9"
    threads: 1
    priority: 50
    resources:
        mem_gb = 22
    shell:
        """
        if [ -s {input} ]
        then
            fastqc --extract --outdir {params.outdir} {input}
        else
            touch {output.html}
        fi

        """

rule find_low_count_fqs:
    """
    Check fastq files to find those with low read counts.
    """
    input:
        expand("Data/Intensities/BaseCalls/{sample.Sample_Project}/FastQC/{sample.Sample_Name}_L000_R{read}_001_fastqc/done", sample=samples.itertuples(), read=['1','2'])
    output:
        low_counts_yaml=low_counts_fq_yamls,
        low_counts_log="low_count_fastqs.log"
    params:
        min_reads=min_reads
    log:
        stdout="snakemake_job_logs/find_low_count_fqs/out.o",
        stderr="snakemake_job_logs/find_low_count_fqs/err.e"
    benchmark:
        "snakemake_job_logs/benchmarks/find_low_count_fqs/bench.txt"
    envmodules:
    threads: 1
    priority: 50
    resources:
        mem_gb = 22
    shell:
        """
        # Add header lines for the low_counts fastqs yaml
        for yaml_file in {output.low_counts_yaml}
        do
            echo "id: 'low-counts-fqs'\nsection_name: 'Low count fastq files after dmux'\ndescription: 'This section lists all the fastq files with < {params.min_reads} reads after bcl2fastq and lane-merging.'\nplot_type: 'html'\ndata: |\n    <ul>" > $yaml_file
        done

        # Add low_counts files to the low_counts fastqs yaml files
        for fastqc_done in {input}
        do
            fastqc_data_file=`dirname $fastqc_done`'/fastqc_data.txt'
            fastq=$(basename $(dirname $fastqc_done) | perl -npe 's/_fastqc/.fastq.gz/')
            proj_dir=$(dirname $(dirname $(dirname $fastqc_data_file)))

            if [ -e $fastqc_data_file ]
            then
                num_reads=$(grep -P '^Total Sequences' $fastqc_data_file | grep -Po '\d+$')
            else
                num_reads=0
            fi

            # Check the number of reads
            if [ $num_reads -lt {params.min_reads} ]
            then
                echo "$proj_dir/$fastq\t$num_reads" >> {output.low_counts_log}
                echo "      <li>$fastq\t$num_reads</li>" >> "$proj_dir/low_counts_fastqs_mqc.yaml"
            fi
        done
                
        # Add footer line for the low_counts fastqs yaml
        for yaml_file in {output.low_counts_yaml}
        do
            echo "    </ul>" >> $yaml_file
        done     
        # if all of the fastq files have more than 100 reads, then the snakemake pipeline is not able to generate the low_count_fastqs.log file.
        # So the below command is added.
        echo "done" >> {output.low_counts_log}
        """


# rule fastqc_undetermined:
#     """
#     Run fastqc for merged (across lanes) undetermined files.
#     """
#     input:
#         "Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001.fastq.gz"
#     output:
#         html="Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001_fastqc.html",
#         zip="Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001_fastqc.zip"
#     params:
#         outdir="Data/Intensities/BaseCalls/"
#     log:
#         stdout="snakemake_job_logs/fastqc/Undetermined_L000_R{read}.o",
#         stderr="snakemake_job_logs/fastqc/Undetermined_L000_R{read}.e"
#     benchmark:
#         "snakemake_job_logs/benchmarks/fastqc/Undetermined_L000_R{read}.txt"
#     envmodules:
#         "bbc/fastqc/fastqc-0.11.9"
#     threads: 1
#     resources:
#         mem_gb = 22
#     shell:
#         """
#         fastqc --outdir {params.outdir} {input}
#         """

rule fastq_screen:
    """
    Run fastq_screen for merged (across lanes) sample files.
    """
    input:
        "Data/Intensities/BaseCalls/{project}/{sample}_L000_R{read}_001.fastq.gz"
    output:
        html = "Data/Intensities/BaseCalls/{project}/FastQC/{sample}_L000_R{read}_001_screen.html",
        txt = "Data/Intensities/BaseCalls/{project}/FastQC/{sample}_L000_R{read}_001_screen.txt",
    params:
        outdir = "Data/Intensities/BaseCalls/{project}/FastQC/"
    log:
        stdout="snakemake_job_logs/fastq_screen/{project}/{sample}_R{read}.o",
        stderr="snakemake_job_logs/fastq_screen/{project}/{sample}_R{read}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/fastq_screen/{project}/{sample}_R{read}.txt"
    envmodules:
        "bbc/fastq_screen/fastq_screen-0.14.0"
    threads: 4
    resources:
        mem_gb = 88
    shell:
        """
        if [ -s {input} ]
        then
            fastq_screen --threads {threads} --outdir {params.outdir} {input}
        else
            touch {output.html} {output.txt}
        fi
        """

# rule fastq_screen_undetermined:
#     """
#     Run fastq_screen for merged (across lanes) sample files.
#     """
#     input:
#         "Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001.fastq.gz"
#     output:
#         html = "Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001_screen.html",
#         txt = "Data/Intensities/BaseCalls/Undetermined_L000_R{read}_001_screen.txt",
#     params:
#         outdir = "Data/Intensities/BaseCalls/"
#     log:
#         stdout="snakemake_job_logs/fastq_screen/Undetermined_L000_R{read}.o",
#         stderr="snakemake_job_logs/fastq_screen/Undetermined_L000_R{read}.e"
#     benchmark:
#         "snakemake_job_logs/benchmarks/fastq_screen/Undetermined_L000_R{read}.txt"
#     envmodules:
#         "bbc/fastq_screen/fastq_screen-0.14.0"
#     threads: 4
#     resources:
#         mem_gb = 88
#     shell:
#         """
#         fastq_screen --threads {threads} --outdir {params.outdir} {input}
#         """

rule bwa:
    input:
        expand("Data/Intensities/BaseCalls/{{project}}/{{sample}}_L000_R{read}_001.fastq.gz", read=["1","2"])
    output:
        outbam=temp("Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam"),
        outbai="Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam.bai",
        idxstat="Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam.idxstat",
        #flagstat="Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam.flagstat"
    log:
        stdout="snakemake_job_logs/bwa/{project}/{sample}.o",
        stderr="snakemake_job_logs/bwa/{project}/{sample}.e",
    benchmark:
        "snakemake_job_logs/benchmarks/bwa/{project}/{sample}.txt"
    params:
        bwa_idx=config['complexity']['refs'][complexity_ref_name]['bwa']['index'],
    threads: 4
    envmodules:
        "bbc/bwa/bwa-0.7.17",
        "bbc/samtools/samtools-1.12",
    resources:
        mem_gb=88
    shell:
        """
        bwa mem \
        -t {threads} \
        {params.bwa_idx} \
        {input} | \
        samtools sort \
        -m 3G \
        -@ {threads} \
        -O "BAM" \
        -o {output.outbam} \
        -

        echo "END bwa" >> {log.stdout}
        echo "END bwa" >> {log.stderr}

        samtools index -@ {threads} {output.outbam}

        echo "END indexing" >> {log.stdout}
        echo "END indexing" >> {log.stderr}
        
        samtools idxstats -@ {threads} {output.outbam} > {output.idxstat}
        echo "END idxstats" >> {log.stdout}
        echo "END idxstats" >> {log.stderr}
        
        """

rule star:
    input:
        expand("Data/Intensities/BaseCalls/{{project}}/{{sample}}_L000_R{read}_001.fastq.gz", read=["1","2"])
    output:
        outbam =              temp("Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Aligned.sortedByCoord.out.bam"),
        bai =                 "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Aligned.sortedByCoord.out.bam.bai",
        log_final =           "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Log.final.out",
        log =                 "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Log.out",
        sj =                  "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.SJ.out.tab",
        g_dir =               directory("Data/Intensities/BaseCalls/{project}/Align/star/{sample}._STARgenome"),
        pass1_dir =           directory("Data/Intensities/BaseCalls/{project}/Align/star/{sample}._STARpass1"),
        idxstat =             "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Aligned.sortedByCoord.out.bam.idxstat"
    log:
        stdout = "snakemake_job_logs/star/{project}/{sample}.o",
        stderr = "snakemake_job_logs/star/{project}/{sample}.e",
    benchmark:
        "snakemake_job_logs/benchmarks/star/{project}/{sample}.txt"
    params:
        index = config['complexity']['refs'][complexity_ref_name]['star']['index'],
        outprefix = "Data/Intensities/BaseCalls/{project}/Align/star/{sample}."
    threads: 4
    envmodules:
        "bbc/STAR/STAR-2.7.8a",
        "bbc/samtools/samtools-1.12",
    resources:
        mem_gb=88
    shell:
        """
        STAR \
        --runThreadN {threads} \
        --genomeDir {params.index} \
        --readFilesIn {input} \
        --twopassMode Basic \
        --readFilesCommand zcat \
        --outSAMtype BAM SortedByCoordinate \
        --outFileNamePrefix {params.outprefix} \
        --outStd Log 2> {log.stderr}

        echo "END star" >> {log.stdout}
        echo "END star" >> {log.stderr}

        samtools index -@ {threads} {output.outbam}

        echo "END indexing" >> {log.stdout}
        echo "END indexing" >> {log.stderr}
        
        samtools idxstats {output.outbam} > {output.idxstat}

        echo "END idxstats" >> {log.stdout}
        echo "END idxstats" >> {log.stderr}
 
        """


rule CollectAlignmentSummaryMetrics:
    """
    Run Picard CollectAlignmentSummaryMetrics.
    """
    input:
        "Data/Intensities/BaseCalls/{project}/Align/{aligner}/{sample}.Aligned.sortedByCoord.out.bam" if config['complexity']['DNA_or_RNA']=='RNA' else "Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam" 
    output:
        out="Data/Intensities/BaseCalls/{project}/Align/{aligner}/CollectAlignmentSummaryMetrics/{sample}.bam.aln_metrics"
    params:
        temp="Data/Intensities/BaseCalls/{project}/Align/{aligner}/CollectAlignmentSummaryMetrics/",
        reffasta=lambda wildcards: config['complexity']['refs'][complexity_ref_name][wildcards.aligner]["fasta"]
    log:
        stdout="snakemake_job_logs/{aligner}/CollectAlignmentSummaryMetrics/{project}/{sample}.o",
        stderr="snakemake_job_logs/{aligner}/CollectAlignmentSummaryMetrics/{project}/{sample}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/{aligner}/CollectAlignmentSummaryMetrics/{project}/{sample}.txt"
    envmodules:
        "bbc/picard/picard-2.23.3"
    threads: 4
    resources: 
        mem_gb = 64
    shell:
        """
        java -Xms8g -Xmx{resources.mem_gb}g -Djava.io.tmpdir={params.temp} -jar $PICARD CollectAlignmentSummaryMetrics -I {input} -O {output.out} -R {params.reffasta}  
        """

rule preseq_complexity:
    """
    Run preseq c_curve and lc_extrap on the BAMs.
    """
    input:
        "Data/Intensities/BaseCalls/{project}/Align/star/{sample}.Aligned.sortedByCoord.out.bam" if config['complexity']['DNA_or_RNA']=='RNA' else "Data/Intensities/BaseCalls/{project}/Align/bwa/{sample}.bam" 
    output:
        filtbam=temp("Data/Intensities/BaseCalls/{project}/Align/preseq_complexity/{sample}.filt.bam"),
        ccurve="Data/Intensities/BaseCalls/{project}/Align/preseq_complexity/{sample}.c_curve.txt",
        lcextrap="Data/Intensities/BaseCalls/{project}/Align/preseq_complexity/{sample}.lc_extrap.txt",
    log:
        stdout="snakemake_job_logs/preseq_complexity/{project}/{sample}.o",
        stderr="snakemake_job_logs/preseq_complexity/{project}/{sample}.e"
    benchmark:
        "snakemake_job_logs/benchmarks/preseq_complexity/{project}/{sample}.txt"
    envmodules:
        "bbc/preseq/preseq-3.1.2",
        "bbc/samtools/samtools-1.12"
    params:
        min_reads=max(min_reads, config['complexity']['preseq_min_reads']),
        seg_len_max=config['complexity']['preseq_seq_len_max']#10000000,
    resources:
        mem_gb=88
    threads: 4
    shell:
        """
        # Filter for primary alignments only
        samtools view -@ {threads} -F 256 -o {output.filtbam} {input}

        # Run preseq only if there are more reads than the cutoff
        if [ `samtools view -f 64 -F 256 -c {output.filtbam}` -lt {params.min_reads} ]
        then
            touch {output.ccurve} {output.lcextrap}
        else

            preseq c_curve \
            -l {params.seg_len_max} \
            -v \
            -P \
            -bam \
            -o {output.ccurve} \
            {output.filtbam}

            echo "Finished c_curve." >&1
            echo "Finished c_curve." >&2

            preseq lc_extrap \
            -l {params.seg_len_max} \
            -v \
            -P \
            -bam \
            -o {output.lcextrap} \
            {output.filtbam}

            echo "Finished lc_extrap." >&1
            echo "Finished lc_extrap." >&2
        fi
        """
